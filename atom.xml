<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://choara.github.io</id>
    <title>choara&apos;s blog</title>
    <updated>2020-11-30T00:16:12.019Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://choara.github.io"/>
    <link rel="self" href="https://choara.github.io/atom.xml"/>
    <subtitle>心有猛虎 细嗅蔷薇</subtitle>
    <logo>https://choara.github.io/images/avatar.png</logo>
    <icon>https://choara.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, choara&apos;s blog</rights>
    <entry>
        <title type="html"><![CDATA[Spark Checkpoint]]></title>
        <id>https://choara.github.io/post/spark-checkpoint/</id>
        <link href="https://choara.github.io/post/spark-checkpoint/">
        </link>
        <updated>2020-11-30T00:11:34.000Z</updated>
        <content type="html"><![CDATA[<h2 id="checkpoint">CheckPoint</h2>
<p>Spark离线计算中通过checkpoint缓存RDD数据，切断过长的Lineage，任务失败不用重新从源头计算，直接从checkpoint恢复。checkpoint会重新起个任务进行计算存储，因为spark DAG的机制，不会保存中间状态的数据，所以checkpoint前建议cache，将对应的结果缓存到内存或磁盘中，直接从cache中持久化。</p>
<p>Spark流式计算中存储两类数据，元数据：1. 配置-创建流应用程序的配置。2. DStream操作-DStream操作集。3. 数据batch-接收但未完成的批次。数据：流处理中有窗口或状态操作，根据窗口长度、checkpoint时间点需要对历史数据进行存储，存储只保留 最近几个必要的时间点。</p>
<p>离线和实时的区别在于离线只存储了数据，用于在计算中某个计算失败数据丢失，可以从checkpoint点重新开始计算。实时可以从checkpoint中恢复任务，除了已完成的数据、未完成的数据，还存储了应用程序的配置、对数据流的操作，合适的 checkpoint 能在保证吞吐降低不多（checkpoint另起一个任务将数据持久化）的情况下保证程序的稳定，DStream 5-10 个滑动间隔是合适的。</p>
<h3 id="源码">源码</h3>
<pre><code class="language-scala">// 定义 checkpointDir
private[spark] var checkpointDir: Option[String] = None

/**
 * Set the directory under which RDDs are going to be checkpointed. The directory must
 * be a HDFS path if running on a cluster.
 */
def setCheckpointDir(directory: String) {

  // If we are running on a cluster, log a warning if the directory is local.
  // Otherwise, the driver may attempt to reconstruct the checkpointed RDD from
  // its own local file system, which is incorrect because the checkpoint files
  // are actually on the executor machines.
  // 如果运行的是 cluster 模式，当设置本地文件夹的时候，会报 warning
  // 道理很简单，被创建出来的文件夹路径实际上是 executor 本地的文件夹路径，不是不行，
  // 只是有点不合理，Checkpoint 的东西最好还是放在分布式的文件系统中
  if (!isLocal &amp;&amp; Utils.nonLocalPaths(directory).isEmpty) {
    logWarning(&quot;Spark is not running in local mode, therefore the checkpoint directory &quot; +
      s&quot;must not be on the local filesystem. Directory '$directory' &quot; +
      &quot;appears to be on the local filesystem.&quot;)
  }

  checkpointDir = Option(directory).map { dir =&gt;
    // 显然文件夹名就是 UUID.randoUUID() 生成的
    val path = new Path(dir, UUID.randomUUID().toString)
    val fs = path.getFileSystem(hadoopConfiguration)
    fs.mkdirs(path)
    fs.getFileStatus(path).getPath.toString
  }
}
</code></pre>
<p><code>setCheckpointDir</code> 被那些类调用了。除了常见的 <code>StreamingContext</code> 中需要使用（因为容错性是流式计算的基本保证），另外的就是一些需要反复迭代计算使用 RDD 的场景，包括各种机器学习算法的时候，图中可以看到像 ALS, Decision Tree 等等算法，这些算法往往需要反复使用 RDD，遇到大的数据集用 Cache 就没有什么意义了，所以一般会用 Checkpoint。</p>
<p><code>runJob</code> 方法的最后是一个 <code>rdd.toCheckPoint()</code> 的使用。<code>runJob</code> 是触发 action 的方法。</p>
<pre><code class="language-scala">/**
 * Run a function on a given set of partitions in an RDD and pass the results to the given
 * handler function. This is the main entry point for all actions in Spark.
 */
def runJob[T, U: ClassTag](
    rdd: RDD[T],
    func: (TaskContext, Iterator[T]) =&gt; U,
    partitions: Seq[Int],
    resultHandler: (Int, U) =&gt; Unit): Unit = {
  if (stopped.get()) {
    throw new IllegalStateException(&quot;SparkContext has been shutdown&quot;)
  }
  val callSite = getCallSite
  val cleanedFunc = clean(func)
  logInfo(&quot;Starting job: &quot; + callSite.shortForm)
  if (conf.getBoolean(&quot;spark.logLineage&quot;, false)) {
    logInfo(&quot;RDD's recursive dependencies:\n&quot; + rdd.toDebugString)
  }
  dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)
  progressBar.foreach(_.finishAll())
  rdd.doCheckpoint()
}
</code></pre>
<p><code>doCheckpoint()</code> 是 <code>RDD</code> 的私有方法，到底是 <code>Checkpoint</code> 什么。答案就是 RDD。</p>
<pre><code class="language-scala">/**
 * Performs the checkpointing of this RDD by saving this. It is called after a job using this RDD
 * has completed (therefore the RDD has been materialized and potentially stored in memory).
 * doCheckpoint() is called recursively on the parent RDDs.
 */
// 显然 checkpoint 是在使用完前一个 RDD 之后才会被执行的操作
private[spark] def doCheckpoint(): Unit = {
  RDDOperationScope.withScope(sc, &quot;checkpoint&quot;, allowNesting = false, ignoreParent = true) {
    if (!doCheckpointCalled) {
      doCheckpointCalled = true
      if (checkpointData.isDefined) {
        if (checkpointAllMarkedAncestors) {
          // TODO We can collect all the RDDs that needs to be checkpointed, and then checkpoint
          // them in parallel.
          // Checkpoint parents first because our lineage will be truncated after we
          // checkpoint ourselves
          dependencies.foreach(_.rdd.doCheckpoint())
        }
        checkpointData.get.checkpoint()
      } else {
        dependencies.foreach(_.rdd.doCheckpoint())
      }
    }
  }
}
</code></pre>
<p>上面代码可以看到，需要判断一下一个变量 <code>checkpointData</code> 是否为空。那么它是这么被定义的。</p>
<pre><code class="language-scala">private[spark] var checkpointData: Option[RDDCheckpointData[T]] = None
</code></pre>
<p>然后看看 <code>RDDCheckPointData</code> 是个什么样的数据结构。</p>
<pre><code class="language-scala">/**
 * This class contains all the information related to RDD checkpointing. Each instance of this
 * class is associated with an RDD. It manages process of checkpointing of the associated RDD,
 * as well as, manages the post-checkpoint state by providing the updated partitions,
 * iterator and preferred locations of the checkpointed RDD.
 */
private[spark] abstract class RDDCheckpointData[T: ClassTag](@transient private val rdd: RDD[T])
  extends Serializable {
  import CheckpointState._
  // The checkpoint state of the associated RDD.
  protected var cpState = Initialized
  // The RDD that contains our checkpointed data
  // 显然，这个就是被 Checkpoint 的 RDD 的数据
  private var cpRDD: Option[CheckpointRDD[T]] = None
  // TODO: are we sure we need to use a global lock in the following methods?
  /**
   * Return whether the checkpoint data for this RDD is already persisted.
   */
  def isCheckpointed: Boolean = RDDCheckpointData.synchronized {
    cpState == Checkpointed
  }
  /**
   * Materialize this RDD and persist its content.
   * This is called immediately after the first action invoked on this RDD has completed.
   */
  final def checkpoint(): Unit = {
    // Guard against multiple threads checkpointing the same RDD by
    // atomically flipping the state of this RDDCheckpointData
    RDDCheckpointData.synchronized {
      if (cpState == Initialized) {
        cpState = CheckpointingInProgress
      } else {
        return
      }
    }
    val newRDD = doCheckpoint()
    // Update our state and truncate the RDD lineage
    // 可以看到 cpRDD 在此处被赋值，通过 newRDD 来生成，而生成的方法是 doCheckpointa()
    RDDCheckpointData.synchronized {
      cpRDD = Some(newRDD)
      cpState = Checkpointed
      rdd.markCheckpointed()
    }
  }
  /**
   * Materialize this RDD and persist its content.
   *
   * Subclasses should override this method to define custom checkpointing behavior.
   * @return the checkpoint RDD created in the process.
   */
   // 这个是 Checkpoint RDD 的抽象方法
  protected def doCheckpoint(): CheckpointRDD[T]
  /**
   * Return the RDD that contains our checkpointed data.
   * This is only defined if the checkpoint state is `Checkpointed`.
   */
  def checkpointRDD: Option[CheckpointRDD[T]] = RDDCheckpointData.synchronized { cpRDD }
  /**
   * Return the partitions of the resulting checkpoint RDD.
   * For tests only.
   */
  def getPartitions: Array[Partition] = RDDCheckpointData.synchronized {
    cpRDD.map(_.partitions).getOrElse { Array.empty }
  }
}
</code></pre>
<p>根据注释，可以知道这个类涵盖了 RDD Checkpoint 的所有信息。除了控制 Checkpoint 的过程，还会处理之后的状态变更。</p>
<pre><code class="language-scala">/**
 * Enumeration to manage state transitions of an RDD through checkpointing
 *
 * [ Initialized --{@literal &gt;} checkpointing in progress --{@literal &gt;} checkpointed ]
 */
private[spark] object CheckpointState extends Enumeration {
  type CheckpointState = Value
  val Initialized, CheckpointingInProgress, Checkpointed = Value
}
</code></pre>
<p>显然 Checkpoint 的过程分为初始化[Initialized] -&gt; 正在 Checkpoint[CheckpointingInProgress] -&gt; 结束 Checkpoint[Checkpointed] 三种状态。</p>
<p>关于 <code>RDDCheckpointData</code> 有两个实现，分别分析一下。</p>
<ol>
<li>LocalRDDCheckpointData: RDD 会被保存到 Executor 本地文件系统中，以减少保存到分布式容错性文件系统的巨额开销，因此 Local 形式的 Checkpoint 是基于持久化来做的，没有写到外部分布式文件系统。</li>
<li>ReliableRDDCheckpointData: Reliable 很好理解，就是把 RDD Checkpoint 到可依赖的文件系统，言下之意就是 Driver 重启的时候也可以从失败的时间点进行恢复，无需再走一次 RDD 的转换过程。</li>
</ol>
<h4 id="localrddcheckpointdata">LocalRDDCheckpointData</h4>
<p>LocalRDDCheckpointData 中的核心方法 <code>doCheckpoint()</code>。需要保证 RDD 用了 <code>useDisk</code> 级别的持久化。需要运行一个 Spark 任务来重新构建这个 RDD。最终 new 一个 LocalCheckpointRDD 实例。</p>
<pre><code class="language-scala">......
/**
 * Ensure the RDD is fully cached so the partitions can be recovered later.
 */
protected override def doCheckpoint(): CheckpointRDD[T] = {
  val level = rdd.getStorageLevel

  // Assume storage level uses disk; otherwise memory eviction may cause data loss
  assume(level.useDisk, s&quot;Storage level $level is not appropriate for local checkpointing&quot;)

  // Not all actions compute all partitions of the RDD (e.g. take). For correctness, we
  // must cache any missing partitions. TODO: avoid running another job here (SPARK-8582).
  val action = (tc: TaskContext, iterator: Iterator[T]) =&gt; Utils.getIteratorSize(iterator)
  val missingPartitionIndices = rdd.partitions.map(_.index).filter { i =&gt;
    !SparkEnv.get.blockManager.master.contains(RDDBlockId(rdd.id, i))
  }
  if (missingPartitionIndices.nonEmpty) {
    rdd.sparkContext.runJob(rdd, action, missingPartitionIndices)
  }

  new LocalCheckpointRDD[T](rdd)
}
.....
</code></pre>
<h4 id="reliablerddcheckpointdata">ReliableRDDCheckpointData</h4>
<p>这个是写外部文件系统的 Checkpoint 类。</p>
<pre><code class="language-scala">......
/**
 * Materialize this RDD and write its content to a reliable DFS.
 * This is called immediately after the first action invoked on this RDD has completed.
 */
protected override def doCheckpoint(): CheckpointRDD[T] = {
  val newRDD = ReliableCheckpointRDD.writeRDDToCheckpointDirectory(rdd, cpDir)

  // Optionally clean our checkpoint files if the reference is out of scope
  if (rdd.conf.getBoolean(&quot;spark.cleaner.referenceTracking.cleanCheckpoints&quot;, false)) {
    rdd.context.cleaner.foreach { cleaner =&gt;
      cleaner.registerRDDCheckpointDataForCleanup(newRDD, rdd.id)
    }
  }

  logInfo(s&quot;Done checkpointing RDD ${rdd.id} to $cpDir, new parent is RDD ${newRDD.id}&quot;)
  newRDD
}
......
</code></pre>
<p>可以看到核心方法是通过 <code>ReliableCheckpointRDD.writeRDDToCheckpointDirectory()</code> 来写 <code>newRDD</code>。这个方法就不进去看了，代码逻辑非常清晰，同样是起一个 Spark 任务把 RDD 生成之后按 Partition 来写到文件系统中。</p>
<h3 id="目录">目录</h3>
<p>Rdd</p>
<pre><code class="language-scala">checkpointDir/
|--	uuid_dir
|		|--part1
|--	|--part2
</code></pre>
<p>DStream</p>
<pre><code class="language-scala">checkpointDir/
|-- 6f53f0dd-9461-4ca5-b42c-127739e0853f RDD checkpoint数据
|		|-- rdd-924
|		|		|-- part1
|		|		|-- part2
|		|-- rdd-929
|--	receivedBlockMetadata WAL receiver block 元数据
|		|-- log-1597377346017-1597377406017
|		|-- log-1597377410005-1597377470005
|-- receivedData WAL data
|   |-- 0(streamid)
|   |   |-- log-starttime0-endtime0
|   |   |-- log-starttime1-endtime1
|   |   |-- log-1597378278202-1597378338202
|   |-- streamId1
|   |-- streamId2
|-- .checkpoint-1597378750000.bk.crc
|-- .checkpoint-1597378750000.crc
|-- checkpoint-1597378710000 metadata、data checkpoint(以wordcount为例，看到checkpoint文件大小随着需要保存的状态变多而变大，所以除了元数据也保存了最后合并后的数据本身)
|-- checkpoint-1597378710000.bk 
</code></pre>
]]></content>
    </entry>
</feed>